<div id="menu" style="position:fixed;top:0;left:0;width:100%;background:#f8f8f8;border-bottom:1px solid #ddd;padding:5px 0;z-index:1000;text-align:center;">
  <a href="index.html" style="margin:0 15px;color:#800080;font-weight:bold;">Home</a>
  <a href="topic-description.html" style="margin:0 15px;color:#800080;font-weight:bold;">Topic Description</a>
  <a href="methodology.html" style="margin:0 15px;color:#800080;font-weight:bold;">Gap, Methodology and Tools</a>
  <a href="steps.html" style="margin:0 15px;color:#800080;font-weight:bold;">Methodological Steps</a>
  <a href="challenges.html" style="margin:0 15px;color:#800080;font-weight:bold;">Challenges & LLMs Differences</a>
</div>
<p align="center">
  <img src="https://www.farodiroma.it/wp-content/uploads/2022/04/AAAAQf3Io4rXuO2h_zb3O1V4Q8Q4h2ol54FxldOPKIM-v719mG5cbJ8a7UVzbVT3dpNoPyMSBuEwdC7jEHt6u3WJE2831zJ4udb3uXyYpJ5EubMkacQ4y2LUPl7sZMn_nHBVKX5XUPEiW3rZC9_c7uaD1LdMyo.jpeg" alt="Bridgerton" style="display: block; margin: auto; max-width: 100%; height: auto;">
</p>


<br>

# Challenges and LLMs Differences

## Challenges
WIKIDATA VS DBPEDIA <br>
One of the main challenges we faced during the project was deciding between two major knowledge bases: Wikidata (https://www.wikidata.org/wiki/Q85748936) and DBpedia. (https://dbpedia.org/page/Bridgerton) 
Both offer structured semantic data, but they differ significantly in terms of ontology, update frequency (how often the data is refreshed or revised), and level of detail. 
Wikidata provides a more dynamic and community-curated dataset with flexible properties, while DBpedia relies on a more rigid and formally defined ontology extracted from Wikipedia. 
Since our task was to identify gaps and build RDF triples, we ultimately chose DBpedia. This decision was driven by the fact that DBpedia’s fixed ontology and structured format made it easier to detect inconsistencies and missing information.

CHATGPT VS GEMINI 

The queries that Gemini generates use a syntax or format that the query editor does not recognize or support. This means when you try to run these queries in the editor, they produce errors or don’t work as expected. This incompatibility could be due to differences in query language versions, supported features, or syntax rules between Gemini and the query editor.
……..
(DA COMPLETARE)

## LLMs Differences 
Gemini shows weaker understanding of RDFS and makes more semantic errors compared to ChatGPT, especially in zero-shot settings. However, when provided with corrections or few-shot examples, Gemini is able to adjust and produce more accurate responses. Another notable trait is its tendency to explain reasoning processes in detail, even when chain-of-thought explanations are not required by the prompt. 
Gemini tends to provide not only the SPARQL queries but also detailed explanations of how they work inside the queries. While this can be helpful in some cases, it often results in overly verbose and non-necessary responses, though it also reflects the model’s design for transparency in reasoning. This additional material can become confusing, especially when we simply need to copy and paste the query into YASGUI to test it. The extra explanations sometimes make it harder to identify the actual query, slowing down the workflow.
ChatGPT, by contrast, is more concise and to the point. It usually provides a clear and direct answer, focusing on delivering the correct query without unnecessary elaboration—unless specifically requested. This makes it easier and quicker to extract and use the query. Furthermore, the syntax of the queries generated by ChatGPT tends to be more accurate and compatible with SPARQL tools like YASGUI.
